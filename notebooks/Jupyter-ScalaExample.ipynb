{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter-Scala\n",
    "\n",
    "This jupyter-scala kernel is installed following the (non complete yet) documentation in the [official page ](https://github.com/alexarchambault/jupyter-scala/tree/e1e3db4b9f013a2d8b785001ae6a390897bd20fa#classpathadd)\n",
    "\n",
    "This small notebook example shows the following examples:\n",
    "\n",
    "1. A simple scala example\n",
    "2. Importing from maven repositories\n",
    "3. importing libraries into the project\n",
    "4. Working with Spark\n",
    "5. Connecting with Spark to Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "this is a talk test\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(\"test\")\n",
    "println(\"this is a talk test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mcompare\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compare(a:Int, b:Int):Boolean = {\n",
    "    a.equals(b)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction \u001b[36mfoo\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def foo(a: String, b: String): String ={\n",
    "    println(a + b)\n",
    "    a.+(b)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres3\u001b[0m: \u001b[32mBoolean\u001b[0m = \u001b[32mfalse\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare(3,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres4\u001b[0m: \u001b[32mBoolean\u001b[0m = \u001b[32mtrue\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string1 String 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres5\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"string1 String 2\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo(\"string1 \", \"String 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m1\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mtrue\u001b[0m"
     ]
    }
   ],
   "source": [
    "show(1)\n",
    "show(compare(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding paths to libraries\n",
    "\n",
    "Sometimes we need to add resources to ivy that are not already in the default configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//classpath.addRepository(\"http://repository.cloudera.com/content/repositories/releases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.addRepository(\"http://dl.bintray.com/scalaz/releases\")\n",
    "classpath.addRepository(\"https://repo1.maven.org/maven2/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading libraries from the repository directly\n",
    "\n",
    "Also there are many libraries that we'll need to be able to work with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 130 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" % \"spark-core_2.11\" % \"1.6.0\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the same as before, the %% adds the scala version at the end and generates the corerct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 130 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" %% \"spark-core\" % \"1.6.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 11 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.spark\" %% \"spark-sql\" % \"1.6.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some issues with Cassandra Spark Connector versions >1.4 where it has a problem with the guava version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 11 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//classpath.addRepository(\"http://spark-packages.org/package/datastax/spark-cassandra-connector\")\n",
    "//classpath.add(\"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"1.6.0-M1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 11 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//classpath.addRepository(\"http://spark-packages.org/package/datastax/spark-cassandra-connector\")\n",
    "//classpath.add(\"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"1.5.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 7 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//classpath.addRepository(\"http://spark-packages.org/package/datastax/spark-cassandra-connector\")\n",
    "classpath.add(\"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"1.3.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 31 artifact(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classpath.add(\"org.apache.cassandra\" % \"cassandra-all\" % \"2.2.5\")\n",
    "//classpath.add(\"org.apache.cassandra\" %% \"cassandra-all\" % \"2.2.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[36morg.apache.spark.{ SparkConf, SparkContext }\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.spark.sql.SQLContext\u001b[0m\n",
       "\u001b[32mimport \u001b[36mcom.datastax.spark.connector._\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.text.SimpleDateFormat\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.util.Calendar\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.util.UUID\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io.File\u001b[0m\n",
       "\u001b[32mimport \u001b[36mjava.io.FileWriter\u001b[0m\n",
       "\u001b[32mimport \u001b[36mscala.io.Source\u001b[0m\n",
       "\u001b[32mimport \u001b[36morg.apache.log4j.PropertyConfigurator\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.{ SparkConf, SparkContext }\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import com.datastax.spark.connector._\n",
    "\n",
    "import java.text.SimpleDateFormat\n",
    "import java.util.Calendar\n",
    "import java.util.UUID\n",
    "import java.io.File\n",
    "import java.io.FileWriter\n",
    "import scala.io.Source\n",
    "import org.apache.log4j.PropertyConfigurator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Spark configuration and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkConf\u001b[0m: \u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@61106360"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//this is an example spark configuration, the setMaster is absolutely needed to be able to connect to the spark service\n",
    "val sparkConf = new SparkConf()\n",
    "      .setAppName(\"JupyterScalaTest\")\n",
    "      .setMaster(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/home/leo/.coursier/cache/v1/https/repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/home/leo/.coursier/cache/v1/https/repo1.maven.org/maven2/ch/qos/logback/logback-classic/1.1.3/logback-classic-1.1.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "16/03/01 18:51:55 INFO SparkContext: Running Spark version 1.6.0\n",
      "16/03/01 18:51:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/03/01 18:51:56 WARN Utils: Your hostname, deepl resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "16/03/01 18:51:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "16/03/01 18:51:56 INFO SecurityManager: Changing view acls to: leo\n",
      "16/03/01 18:51:56 INFO SecurityManager: Changing modify acls to: leo\n",
      "16/03/01 18:51:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(leo); users with modify permissions: Set(leo)\n",
      "16/03/01 18:51:57 INFO Utils: Successfully started service 'sparkDriver' on port 38759.\n",
      "16/03/01 18:51:57 INFO Slf4jLogger: Slf4jLogger started\n",
      "16/03/01 18:51:57 INFO Remoting: Starting remoting\n",
      "16/03/01 18:51:57 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.0.2.15:43059]\n",
      "16/03/01 18:51:57 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 43059.\n",
      "16/03/01 18:51:57 INFO SparkEnv: Registering MapOutputTracker\n",
      "16/03/01 18:51:58 INFO SparkEnv: Registering BlockManagerMaster\n",
      "16/03/01 18:51:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a5af3120-b677-4372-85b9-4117872b8040\n",
      "16/03/01 18:51:58 INFO MemoryStore: MemoryStore started with capacity 1603.1 MB\n",
      "16/03/01 18:51:58 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "16/03/01 18:51:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "16/03/01 18:51:58 INFO SparkUI: Started SparkUI at http://10.0.2.15:4040\n",
      "16/03/01 18:51:58 INFO Executor: Starting executor ID driver on host localhost\n",
      "16/03/01 18:51:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38438.\n",
      "16/03/01 18:51:58 INFO NettyBlockTransferService: Server created on 38438\n",
      "16/03/01 18:51:58 INFO BlockManagerMaster: Trying to register BlockManager\n",
      "16/03/01 18:51:58 INFO BlockManagerMasterEndpoint: Registering block manager localhost:38438 with 1603.1 MB RAM, BlockManagerId(driver, localhost, 38438)\n",
      "16/03/01 18:51:58 INFO BlockManagerMaster: Registered BlockManager\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36msc\u001b[0m: \u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@3a3bfcee"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sc = new SparkContext(sparkConf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a Spark Context instance with which we can play ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Playing a bit with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtextFile\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = MapPartitionsRDD[1] at textFile at Main.scala:32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val textFile = sc.textFile(\"/home/leo/installers/spark-1.6.0-bin-hadoop2.6/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres20\u001b[0m: \u001b[32mLong\u001b[0m = \u001b[32m95L\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textFile.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres21\u001b[0m: \u001b[32mString\u001b[0m = \u001b[32m\"# Apache Spark\"\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "textFile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlinesWithSpark\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[\u001b[32mString\u001b[0m] = MapPartitionsRDD[2] at filter at Main.scala:32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val linesWithSpark = textFile.filter(line => line.contains(\"Spark\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkTextCountLines\u001b[0m: \u001b[32mLong\u001b[0m = \u001b[32m17L\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sparkTextCountLines = linesWithSpark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(sparkTextCountLines )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Playing with Cassandra\n",
    "\n",
    "The examples are based on the official documentation of Datastax Spark Cassandra Connector taken from \n",
    "[here](https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msparkCassaConf\u001b[0m: \u001b[32mSparkConf\u001b[0m = org.apache.spark.SparkConf@220d48e4\n",
       "\u001b[36mscc\u001b[0m: \u001b[32mSparkContext\u001b[0m = org.apache.spark.SparkContext@2ffa5098"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "//Now we create a configuration with the cassandra elements\n",
    "val sparkCassaConf = new SparkConf()\n",
    "      .setAppName(\"JupyterScalaTest-Cassandra\")\n",
    "      .setMaster(\"local\")\n",
    "      .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "      //.set(\"spark.cassandra.connection.port\", \"\")\n",
    "      //.set(\"spark.cassandra.auth.username\", \"user\")\n",
    "      //.set(\"spark.cassandra.auth.password\", \"pwd_cassandra\")\n",
    "\n",
    "val scc = new SparkContext(sparkCassaConf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msqlContext\u001b[0m: \u001b[32mSQLContext\u001b[0m = org.apache.spark.sql.SQLContext@a41f055"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val sqlContext = new SQLContext(scc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrdd\u001b[0m: \u001b[32mrdd\u001b[0m.\u001b[32mCassandraTableScanRDD\u001b[0m[\u001b[32mCassandraRow\u001b[0m] = CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val rdd = scc.cassandraTable(\"test\", \"kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CassandraTableScanRDD[0] at RDD at CassandraRDD.scala:18\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres31\u001b[0m: \u001b[32mColumnSelector\u001b[0m = AllColumns"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.columnNames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres32\u001b[0m: \u001b[32mBoolean\u001b[0m = \u001b[32mfalse\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.isEmpty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres33\u001b[0m: \u001b[32mLong\u001b[0m = \u001b[32m6L\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "CassandraRow{key: key5, value: 5}\n",
      "17.0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(rdd.count)\n",
    "println(rdd.first)\n",
    "println(rdd.map(_.getInt(\"value\")).sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mcollection\u001b[0m: \u001b[32morg\u001b[0m.\u001b[32mapache\u001b[0m.\u001b[32mspark\u001b[0m.\u001b[32mrdd\u001b[0m.\u001b[32mRDD\u001b[0m[(\u001b[32mString\u001b[0m, \u001b[32mInt\u001b[0m)] = ParallelCollectionRDD[5] at parallelize at Main.scala:33"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val collection = scc.parallelize(Seq((\"key10\", 3), (\"key11\", 4)))\n",
    "collection.saveToCassandra(\"test\", \"kv\", SomeColumns(\"key\", \"value\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres36\u001b[0m: \u001b[32mLong\u001b[0m = \u001b[32m8L\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "CassandraRow{key: key5, value: 5}\n",
      "24.0\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(rdd.count)\n",
    "println(rdd.first)\n",
    "println(rdd.map(_.getInt(\"value\")).sum) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "pygments_lexer": "scala",
   "version": "2.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
